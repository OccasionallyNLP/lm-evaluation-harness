{
  "results": {
    "arc_easy": {
      "alias": "arc_easy",
      "acc_norm,none": 0.4,
      "acc_norm_stderr,none": 0.16329931618554522
    },
    "copa": {
      "alias": "copa",
      "acc,none": 0.6,
      "acc_stderr,none": 0.1632993161855452
    },
    "hellaswag": {
      "alias": "hellaswag",
      "acc_norm,none": 0.2,
      "acc_norm_stderr,none": 0.13333333333333333
    },
    "lambada_standard": {
      "alias": "lambada_standard",
      "acc,none": 0.5,
      "acc_stderr,none": 0.16666666666666666
    },
    "logiqa": {
      "alias": "logiqa",
      "acc_norm,none": 0.0,
      "acc_norm_stderr,none": 0.0
    },
    "mmlu": {
      "acc,none": 0.4298245614035088,
      "acc_stderr,none": 0.02045112099420551,
      "alias": "mmlu"
    },
    "mmlu_humanities": {
      "acc,none": 0.4230769230769231,
      "acc_stderr,none": 0.04282895663715401,
      "alias": " - humanities"
    },
    "mmlu_formal_logic": {
      "alias": "  - formal_logic",
      "acc,none": 0.5,
      "acc_stderr,none": 0.16666666666666666
    },
    "mmlu_high_school_european_history": {
      "alias": "  - high_school_european_history",
      "acc,none": 0.5,
      "acc_stderr,none": 0.16666666666666666
    },
    "mmlu_high_school_us_history": {
      "alias": "  - high_school_us_history",
      "acc,none": 0.4,
      "acc_stderr,none": 0.1632993161855452
    },
    "mmlu_high_school_world_history": {
      "alias": "  - high_school_world_history",
      "acc,none": 0.5,
      "acc_stderr,none": 0.16666666666666666
    },
    "mmlu_international_law": {
      "alias": "  - international_law",
      "acc,none": 0.5,
      "acc_stderr,none": 0.16666666666666666
    },
    "mmlu_jurisprudence": {
      "alias": "  - jurisprudence",
      "acc,none": 0.5,
      "acc_stderr,none": 0.16666666666666666
    },
    "mmlu_logical_fallacies": {
      "alias": "  - logical_fallacies",
      "acc,none": 0.6,
      "acc_stderr,none": 0.16329931618554522
    },
    "mmlu_moral_disputes": {
      "alias": "  - moral_disputes",
      "acc,none": 0.1,
      "acc_stderr,none": 0.09999999999999999
    },
    "mmlu_moral_scenarios": {
      "alias": "  - moral_scenarios",
      "acc,none": 0.1,
      "acc_stderr,none": 0.09999999999999999
    },
    "mmlu_philosophy": {
      "alias": "  - philosophy",
      "acc,none": 0.5,
      "acc_stderr,none": 0.16666666666666666
    },
    "mmlu_prehistory": {
      "alias": "  - prehistory",
      "acc,none": 0.3,
      "acc_stderr,none": 0.15275252316519466
    },
    "mmlu_professional_law": {
      "alias": "  - professional_law",
      "acc,none": 0.3,
      "acc_stderr,none": 0.15275252316519464
    },
    "mmlu_world_religions": {
      "alias": "  - world_religions",
      "acc,none": 0.7,
      "acc_stderr,none": 0.15275252316519466
    },
    "mmlu_other": {
      "acc,none": 0.47692307692307695,
      "acc_stderr,none": 0.043815403822141184,
      "alias": " - other"
    },
    "mmlu_business_ethics": {
      "alias": "  - business_ethics",
      "acc,none": 0.3,
      "acc_stderr,none": 0.15275252316519466
    },
    "mmlu_clinical_knowledge": {
      "alias": "  - clinical_knowledge",
      "acc,none": 0.5,
      "acc_stderr,none": 0.16666666666666666
    },
    "mmlu_college_medicine": {
      "alias": "  - college_medicine",
      "acc,none": 0.6,
      "acc_stderr,none": 0.16329931618554522
    },
    "mmlu_global_facts": {
      "alias": "  - global_facts",
      "acc,none": 0.3,
      "acc_stderr,none": 0.15275252316519464
    },
    "mmlu_human_aging": {
      "alias": "  - human_aging",
      "acc,none": 0.6,
      "acc_stderr,none": 0.16329931618554522
    },
    "mmlu_management": {
      "alias": "  - management",
      "acc,none": 0.7,
      "acc_stderr,none": 0.15275252316519466
    },
    "mmlu_marketing": {
      "alias": "  - marketing",
      "acc,none": 0.5,
      "acc_stderr,none": 0.16666666666666666
    },
    "mmlu_medical_genetics": {
      "alias": "  - medical_genetics",
      "acc,none": 0.5,
      "acc_stderr,none": 0.16666666666666666
    },
    "mmlu_miscellaneous": {
      "alias": "  - miscellaneous",
      "acc,none": 0.6,
      "acc_stderr,none": 0.16329931618554522
    },
    "mmlu_nutrition": {
      "alias": "  - nutrition",
      "acc,none": 0.7,
      "acc_stderr,none": 0.15275252316519466
    },
    "mmlu_professional_accounting": {
      "alias": "  - professional_accounting",
      "acc,none": 0.3,
      "acc_stderr,none": 0.15275252316519464
    },
    "mmlu_professional_medicine": {
      "alias": "  - professional_medicine",
      "acc,none": 0.2,
      "acc_stderr,none": 0.13333333333333333
    },
    "mmlu_virology": {
      "alias": "  - virology",
      "acc,none": 0.4,
      "acc_stderr,none": 0.1632993161855452
    },
    "mmlu_social_sciences": {
      "acc,none": 0.43333333333333335,
      "acc_stderr,none": 0.04496226126732957,
      "alias": " - social sciences"
    },
    "mmlu_econometrics": {
      "alias": "  - econometrics",
      "acc,none": 0.2,
      "acc_stderr,none": 0.13333333333333333
    },
    "mmlu_high_school_geography": {
      "alias": "  - high_school_geography",
      "acc,none": 0.3,
      "acc_stderr,none": 0.15275252316519464
    },
    "mmlu_high_school_government_and_politics": {
      "alias": "  - high_school_government_and_politics",
      "acc,none": 0.5,
      "acc_stderr,none": 0.16666666666666666
    },
    "mmlu_high_school_macroeconomics": {
      "alias": "  - high_school_macroeconomics",
      "acc,none": 0.4,
      "acc_stderr,none": 0.1632993161855452
    },
    "mmlu_high_school_microeconomics": {
      "alias": "  - high_school_microeconomics",
      "acc,none": 0.3,
      "acc_stderr,none": 0.15275252316519466
    },
    "mmlu_high_school_psychology": {
      "alias": "  - high_school_psychology",
      "acc,none": 0.7,
      "acc_stderr,none": 0.15275252316519466
    },
    "mmlu_human_sexuality": {
      "alias": "  - human_sexuality",
      "acc,none": 0.6,
      "acc_stderr,none": 0.1632993161855452
    },
    "mmlu_professional_psychology": {
      "alias": "  - professional_psychology",
      "acc,none": 0.6,
      "acc_stderr,none": 0.16329931618554522
    },
    "mmlu_public_relations": {
      "alias": "  - public_relations",
      "acc,none": 0.3,
      "acc_stderr,none": 0.15275252316519466
    },
    "mmlu_security_studies": {
      "alias": "  - security_studies",
      "acc,none": 0.2,
      "acc_stderr,none": 0.13333333333333333
    },
    "mmlu_sociology": {
      "alias": "  - sociology",
      "acc,none": 0.6,
      "acc_stderr,none": 0.16329931618554522
    },
    "mmlu_us_foreign_policy": {
      "alias": "  - us_foreign_policy",
      "acc,none": 0.5,
      "acc_stderr,none": 0.16666666666666666
    },
    "mmlu_stem": {
      "acc,none": 0.4,
      "acc_stderr,none": 0.03464634676865175,
      "alias": " - stem"
    },
    "mmlu_abstract_algebra": {
      "alias": "  - abstract_algebra",
      "acc,none": 0.6,
      "acc_stderr,none": 0.16329931618554522
    },
    "mmlu_anatomy": {
      "alias": "  - anatomy",
      "acc,none": 0.4,
      "acc_stderr,none": 0.16329931618554522
    },
    "mmlu_astronomy": {
      "alias": "  - astronomy",
      "acc,none": 0.7,
      "acc_stderr,none": 0.15275252316519466
    },
    "mmlu_college_biology": {
      "alias": "  - college_biology",
      "acc,none": 0.7,
      "acc_stderr,none": 0.15275252316519466
    },
    "mmlu_college_chemistry": {
      "alias": "  - college_chemistry",
      "acc,none": 0.4,
      "acc_stderr,none": 0.16329931618554522
    },
    "mmlu_college_computer_science": {
      "alias": "  - college_computer_science",
      "acc,none": 0.3,
      "acc_stderr,none": 0.15275252316519466
    },
    "mmlu_college_mathematics": {
      "alias": "  - college_mathematics",
      "acc,none": 0.3,
      "acc_stderr,none": 0.15275252316519466
    },
    "mmlu_college_physics": {
      "alias": "  - college_physics",
      "acc,none": 0.3,
      "acc_stderr,none": 0.15275252316519464
    },
    "mmlu_computer_security": {
      "alias": "  - computer_security",
      "acc,none": 0.7,
      "acc_stderr,none": 0.15275252316519466
    },
    "mmlu_conceptual_physics": {
      "alias": "  - conceptual_physics",
      "acc,none": 0.4,
      "acc_stderr,none": 0.16329931618554522
    },
    "mmlu_electrical_engineering": {
      "alias": "  - electrical_engineering",
      "acc,none": 0.4,
      "acc_stderr,none": 0.16329931618554522
    },
    "mmlu_elementary_mathematics": {
      "alias": "  - elementary_mathematics",
      "acc,none": 0.2,
      "acc_stderr,none": 0.13333333333333333
    },
    "mmlu_high_school_biology": {
      "alias": "  - high_school_biology",
      "acc,none": 0.5,
      "acc_stderr,none": 0.16666666666666666
    },
    "mmlu_high_school_chemistry": {
      "alias": "  - high_school_chemistry",
      "acc,none": 0.6,
      "acc_stderr,none": 0.1632993161855452
    },
    "mmlu_high_school_computer_science": {
      "alias": "  - high_school_computer_science",
      "acc,none": 0.4,
      "acc_stderr,none": 0.16329931618554522
    },
    "mmlu_high_school_mathematics": {
      "alias": "  - high_school_mathematics",
      "acc,none": 0.2,
      "acc_stderr,none": 0.13333333333333333
    },
    "mmlu_high_school_physics": {
      "alias": "  - high_school_physics",
      "acc,none": 0.3,
      "acc_stderr,none": 0.15275252316519464
    },
    "mmlu_high_school_statistics": {
      "alias": "  - high_school_statistics",
      "acc,none": 0.1,
      "acc_stderr,none": 0.09999999999999999
    },
    "mmlu_machine_learning": {
      "alias": "  - machine_learning",
      "acc,none": 0.1,
      "acc_stderr,none": 0.09999999999999999
    },
    "multirc": {
      "alias": "multirc",
      "acc,none": 0.6,
      "acc_stderr,none": 0.16329931618554522
    },
    "openbookqa": {
      "alias": "openbookqa",
      "acc_norm,none": 0.1,
      "acc_norm_stderr,none": 0.09999999999999999
    },
    "piqa": {
      "alias": "piqa",
      "acc_norm,none": 0.7,
      "acc_norm_stderr,none": 0.15275252316519466
    },
    "qqp": {
      "alias": "qqp",
      "acc,none": 0.7,
      "acc_stderr,none": 0.15275252316519466
    },
    "race": {
      "alias": "race",
      "acc,none": 0.3,
      "acc_stderr,none": 0.15275252316519464
    },
    "sciq": {
      "alias": "sciq",
      "acc_norm,none": 0.9,
      "acc_norm_stderr,none": 0.09999999999999999
    },
    "social_iqa": {
      "alias": "social_iqa",
      "acc,none": 0.4,
      "acc_stderr,none": 0.1632993161855452
    },
    "winogrande": {
      "alias": "winogrande",
      "acc,none": 0.5,
      "acc_stderr,none": 0.16666666666666666
    }
  },
  "groups": {
    "mmlu": {
      "acc,none": 0.4298245614035088,
      "acc_stderr,none": 0.02045112099420551,
      "alias": "mmlu"
    },
    "mmlu_humanities": {
      "acc,none": 0.4230769230769231,
      "acc_stderr,none": 0.04282895663715401,
      "alias": " - humanities"
    },
    "mmlu_other": {
      "acc,none": 0.47692307692307695,
      "acc_stderr,none": 0.043815403822141184,
      "alias": " - other"
    },
    "mmlu_social_sciences": {
      "acc,none": 0.43333333333333335,
      "acc_stderr,none": 0.04496226126732957,
      "alias": " - social sciences"
    },
    "mmlu_stem": {
      "acc,none": 0.4,
      "acc_stderr,none": 0.03464634676865175,
      "alias": " - stem"
    }
  },
  "group_subtasks": {
    "winogrande": [],
    "race": [],
    "openbookqa": [],
    "social_iqa": [],
    "hellaswag": [],
    "lambada_standard": [],
    "piqa": [],
    "sciq": [],
    "arc_easy": [],
    "logiqa": [],
    "multirc": [],
    "copa": [],
    "qqp": [],
    "mmlu_humanities": [
      "mmlu_formal_logic",
      "mmlu_high_school_european_history",
      "mmlu_high_school_us_history",
      "mmlu_high_school_world_history",
      "mmlu_international_law",
      "mmlu_jurisprudence",
      "mmlu_logical_fallacies",
      "mmlu_moral_disputes",
      "mmlu_moral_scenarios",
      "mmlu_philosophy",
      "mmlu_prehistory",
      "mmlu_professional_law",
      "mmlu_world_religions"
    ],
    "mmlu_social_sciences": [
      "mmlu_econometrics",
      "mmlu_high_school_geography",
      "mmlu_high_school_government_and_politics",
      "mmlu_high_school_macroeconomics",
      "mmlu_high_school_microeconomics",
      "mmlu_high_school_psychology",
      "mmlu_human_sexuality",
      "mmlu_professional_psychology",
      "mmlu_public_relations",
      "mmlu_security_studies",
      "mmlu_sociology",
      "mmlu_us_foreign_policy"
    ],
    "mmlu_other": [
      "mmlu_business_ethics",
      "mmlu_clinical_knowledge",
      "mmlu_college_medicine",
      "mmlu_global_facts",
      "mmlu_human_aging",
      "mmlu_management",
      "mmlu_marketing",
      "mmlu_medical_genetics",
      "mmlu_miscellaneous",
      "mmlu_nutrition",
      "mmlu_professional_accounting",
      "mmlu_professional_medicine",
      "mmlu_virology"
    ],
    "mmlu_stem": [
      "mmlu_abstract_algebra",
      "mmlu_anatomy",
      "mmlu_astronomy",
      "mmlu_college_biology",
      "mmlu_college_chemistry",
      "mmlu_college_computer_science",
      "mmlu_college_mathematics",
      "mmlu_college_physics",
      "mmlu_computer_security",
      "mmlu_conceptual_physics",
      "mmlu_electrical_engineering",
      "mmlu_elementary_mathematics",
      "mmlu_high_school_biology",
      "mmlu_high_school_chemistry",
      "mmlu_high_school_computer_science",
      "mmlu_high_school_mathematics",
      "mmlu_high_school_physics",
      "mmlu_high_school_statistics",
      "mmlu_machine_learning"
    ],
    "mmlu": [
      "mmlu_stem",
      "mmlu_other",
      "mmlu_social_sciences",
      "mmlu_humanities"
    ]
  },
  "configs": {
    "arc_easy": {
      "task": "arc_easy",
      "tag": [
        "ai2_arc"
      ],
      "dataset_path": "allenai/ai2_arc",
      "dataset_name": "ARC-Easy",
      "training_split": "train",
      "validation_split": "validation",
      "test_split": "test",
      "fewshot_split": "validation",
      "doc_to_text": "Question: {{question}}\nAnswer:",
      "doc_to_target": "{{choices.label.index(answerKey)}}",
      "unsafe_code": false,
      "doc_to_choice": "{{choices.text}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": true,
      "doc_to_decontamination_query": "Question: {{question}}\nAnswer:",
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "copa": {
      "task": "copa",
      "tag": [
        "super-glue-lm-eval-v1"
      ],
      "dataset_path": "super_glue",
      "dataset_name": "copa",
      "training_split": "train",
      "validation_split": "validation",
      "fewshot_split": "train",
      "doc_to_text": "def doc_to_text(doc):\n    # Drop the period\n    connector = {\n        \"cause\": \"because\",\n        \"effect\": \"therefore\",\n    }[doc[\"question\"]]\n    return doc[\"premise\"].strip()[:-1] + f\" {connector}\"\n",
      "doc_to_target": "def doc_to_target(doc):\n    correct_choice = doc[\"choice1\"] if doc[\"label\"] == 0 else doc[\"choice2\"]\n    # Connect the sentences\n    return \" \" + convert_choice(correct_choice)\n",
      "unsafe_code": false,
      "doc_to_choice": "def doc_to_choice(doc):\n    return [\" \" + convert_choice(doc[\"choice1\"]), \" \" + convert_choice(doc[\"choice2\"])]\n",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc"
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "hellaswag": {
      "task": "hellaswag",
      "tag": [
        "multiple_choice"
      ],
      "dataset_path": "Rowan/hellaswag",
      "training_split": "train",
      "validation_split": "validation",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _process_doc(doc):\n        ctx = doc[\"ctx_a\"] + \" \" + doc[\"ctx_b\"].capitalize()\n        out_doc = {\n            \"query\": preprocess(doc[\"activity_label\"] + \": \" + ctx),\n            \"choices\": [preprocess(ending) for ending in doc[\"endings\"]],\n            \"gold\": int(doc[\"label\"]),\n        }\n        return out_doc\n\n    return dataset.map(_process_doc)\n",
      "doc_to_text": "{{query}}",
      "doc_to_target": "{{label}}",
      "unsafe_code": false,
      "doc_to_choice": "choices",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "lambada_standard": {
      "task": "lambada_standard",
      "tag": [
        "lambada"
      ],
      "dataset_path": "lambada",
      "validation_split": "validation",
      "test_split": "test",
      "fewshot_split": "validation",
      "doc_to_text": "{{text.split(' ')[:-1]|join(' ')}}",
      "doc_to_target": "{{' '+text.split(' ')[-1]}}",
      "unsafe_code": false,
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "loglikelihood",
      "repeats": 1,
      "should_decontaminate": true,
      "doc_to_decontamination_query": "{{text}}",
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "logiqa": {
      "task": "logiqa",
      "dataset_path": "xiaoyuanliu/logiqa",
      "training_split": "train",
      "validation_split": "validation",
      "test_split": "test",
      "fewshot_split": "validation",
      "doc_to_text": "def doc_to_text(doc) -> str:\n    \"\"\"\n    Passage: <passage>\n    Question: <question>\n    Choices:\n    A. <choice1>\n    B. <choice2>\n    C. <choice3>\n    D. <choice4>\n    Answer:\n    \"\"\"\n    choices = [\"a\", \"b\", \"c\", \"d\"]\n    prompt = \"Passage: \" + doc[\"context\"] + \"\\n\"\n    prompt += \"Question: \" + doc[\"question\"] + \"\\nChoices:\\n\"\n    for choice, option in zip(choices, doc[\"options\"]):\n        prompt += f\"{choice.upper()}. {option}\\n\"\n    prompt += \"Answer:\"\n    return prompt\n",
      "doc_to_target": "def doc_to_target(doc) -> int:\n    return doc['final_answer']\n",
      "unsafe_code": false,
      "doc_to_choice": "{{options}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": true,
      "doc_to_decontamination_query": "{{context}}",
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_abstract_algebra": {
      "task": "mmlu_abstract_algebra",
      "task_alias": "abstract_algebra",
      "tag": "mmlu_stem_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "abstract_algebra",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about abstract algebra.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_anatomy": {
      "task": "mmlu_anatomy",
      "task_alias": "anatomy",
      "tag": "mmlu_stem_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "anatomy",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about anatomy.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_astronomy": {
      "task": "mmlu_astronomy",
      "task_alias": "astronomy",
      "tag": "mmlu_stem_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "astronomy",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about astronomy.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_business_ethics": {
      "task": "mmlu_business_ethics",
      "task_alias": "business_ethics",
      "tag": "mmlu_other_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "business_ethics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about business ethics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_clinical_knowledge": {
      "task": "mmlu_clinical_knowledge",
      "task_alias": "clinical_knowledge",
      "tag": "mmlu_other_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "clinical_knowledge",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about clinical knowledge.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_college_biology": {
      "task": "mmlu_college_biology",
      "task_alias": "college_biology",
      "tag": "mmlu_stem_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "college_biology",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about college biology.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_college_chemistry": {
      "task": "mmlu_college_chemistry",
      "task_alias": "college_chemistry",
      "tag": "mmlu_stem_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "college_chemistry",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about college chemistry.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_college_computer_science": {
      "task": "mmlu_college_computer_science",
      "task_alias": "college_computer_science",
      "tag": "mmlu_stem_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "college_computer_science",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about college computer science.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_college_mathematics": {
      "task": "mmlu_college_mathematics",
      "task_alias": "college_mathematics",
      "tag": "mmlu_stem_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "college_mathematics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about college mathematics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_college_medicine": {
      "task": "mmlu_college_medicine",
      "task_alias": "college_medicine",
      "tag": "mmlu_other_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "college_medicine",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about college medicine.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_college_physics": {
      "task": "mmlu_college_physics",
      "task_alias": "college_physics",
      "tag": "mmlu_stem_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "college_physics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about college physics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_computer_security": {
      "task": "mmlu_computer_security",
      "task_alias": "computer_security",
      "tag": "mmlu_stem_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "computer_security",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about computer security.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_conceptual_physics": {
      "task": "mmlu_conceptual_physics",
      "task_alias": "conceptual_physics",
      "tag": "mmlu_stem_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "conceptual_physics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about conceptual physics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_econometrics": {
      "task": "mmlu_econometrics",
      "task_alias": "econometrics",
      "tag": "mmlu_social_sciences_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "econometrics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about econometrics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_electrical_engineering": {
      "task": "mmlu_electrical_engineering",
      "task_alias": "electrical_engineering",
      "tag": "mmlu_stem_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "electrical_engineering",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about electrical engineering.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_elementary_mathematics": {
      "task": "mmlu_elementary_mathematics",
      "task_alias": "elementary_mathematics",
      "tag": "mmlu_stem_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "elementary_mathematics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about elementary mathematics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_formal_logic": {
      "task": "mmlu_formal_logic",
      "task_alias": "formal_logic",
      "tag": "mmlu_humanities_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "formal_logic",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about formal logic.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_global_facts": {
      "task": "mmlu_global_facts",
      "task_alias": "global_facts",
      "tag": "mmlu_other_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "global_facts",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about global facts.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_high_school_biology": {
      "task": "mmlu_high_school_biology",
      "task_alias": "high_school_biology",
      "tag": "mmlu_stem_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "high_school_biology",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about high school biology.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_high_school_chemistry": {
      "task": "mmlu_high_school_chemistry",
      "task_alias": "high_school_chemistry",
      "tag": "mmlu_stem_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "high_school_chemistry",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about high school chemistry.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_high_school_computer_science": {
      "task": "mmlu_high_school_computer_science",
      "task_alias": "high_school_computer_science",
      "tag": "mmlu_stem_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "high_school_computer_science",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about high school computer science.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_high_school_european_history": {
      "task": "mmlu_high_school_european_history",
      "task_alias": "high_school_european_history",
      "tag": "mmlu_humanities_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "high_school_european_history",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about high school european history.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_high_school_geography": {
      "task": "mmlu_high_school_geography",
      "task_alias": "high_school_geography",
      "tag": "mmlu_social_sciences_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "high_school_geography",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about high school geography.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_high_school_government_and_politics": {
      "task": "mmlu_high_school_government_and_politics",
      "task_alias": "high_school_government_and_politics",
      "tag": "mmlu_social_sciences_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "high_school_government_and_politics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about high school government and politics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_high_school_macroeconomics": {
      "task": "mmlu_high_school_macroeconomics",
      "task_alias": "high_school_macroeconomics",
      "tag": "mmlu_social_sciences_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "high_school_macroeconomics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about high school macroeconomics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_high_school_mathematics": {
      "task": "mmlu_high_school_mathematics",
      "task_alias": "high_school_mathematics",
      "tag": "mmlu_stem_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "high_school_mathematics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about high school mathematics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_high_school_microeconomics": {
      "task": "mmlu_high_school_microeconomics",
      "task_alias": "high_school_microeconomics",
      "tag": "mmlu_social_sciences_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "high_school_microeconomics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about high school microeconomics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_high_school_physics": {
      "task": "mmlu_high_school_physics",
      "task_alias": "high_school_physics",
      "tag": "mmlu_stem_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "high_school_physics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about high school physics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_high_school_psychology": {
      "task": "mmlu_high_school_psychology",
      "task_alias": "high_school_psychology",
      "tag": "mmlu_social_sciences_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "high_school_psychology",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about high school psychology.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_high_school_statistics": {
      "task": "mmlu_high_school_statistics",
      "task_alias": "high_school_statistics",
      "tag": "mmlu_stem_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "high_school_statistics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about high school statistics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_high_school_us_history": {
      "task": "mmlu_high_school_us_history",
      "task_alias": "high_school_us_history",
      "tag": "mmlu_humanities_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "high_school_us_history",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about high school us history.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_high_school_world_history": {
      "task": "mmlu_high_school_world_history",
      "task_alias": "high_school_world_history",
      "tag": "mmlu_humanities_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "high_school_world_history",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about high school world history.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_human_aging": {
      "task": "mmlu_human_aging",
      "task_alias": "human_aging",
      "tag": "mmlu_other_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "human_aging",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about human aging.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_human_sexuality": {
      "task": "mmlu_human_sexuality",
      "task_alias": "human_sexuality",
      "tag": "mmlu_social_sciences_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "human_sexuality",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about human sexuality.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_international_law": {
      "task": "mmlu_international_law",
      "task_alias": "international_law",
      "tag": "mmlu_humanities_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "international_law",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about international law.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_jurisprudence": {
      "task": "mmlu_jurisprudence",
      "task_alias": "jurisprudence",
      "tag": "mmlu_humanities_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "jurisprudence",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about jurisprudence.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_logical_fallacies": {
      "task": "mmlu_logical_fallacies",
      "task_alias": "logical_fallacies",
      "tag": "mmlu_humanities_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "logical_fallacies",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about logical fallacies.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_machine_learning": {
      "task": "mmlu_machine_learning",
      "task_alias": "machine_learning",
      "tag": "mmlu_stem_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "machine_learning",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about machine learning.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_management": {
      "task": "mmlu_management",
      "task_alias": "management",
      "tag": "mmlu_other_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "management",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about management.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_marketing": {
      "task": "mmlu_marketing",
      "task_alias": "marketing",
      "tag": "mmlu_other_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "marketing",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about marketing.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_medical_genetics": {
      "task": "mmlu_medical_genetics",
      "task_alias": "medical_genetics",
      "tag": "mmlu_other_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "medical_genetics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about medical genetics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_miscellaneous": {
      "task": "mmlu_miscellaneous",
      "task_alias": "miscellaneous",
      "tag": "mmlu_other_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "miscellaneous",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about miscellaneous.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_moral_disputes": {
      "task": "mmlu_moral_disputes",
      "task_alias": "moral_disputes",
      "tag": "mmlu_humanities_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "moral_disputes",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about moral disputes.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_moral_scenarios": {
      "task": "mmlu_moral_scenarios",
      "task_alias": "moral_scenarios",
      "tag": "mmlu_humanities_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "moral_scenarios",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about moral scenarios.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_nutrition": {
      "task": "mmlu_nutrition",
      "task_alias": "nutrition",
      "tag": "mmlu_other_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "nutrition",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about nutrition.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_philosophy": {
      "task": "mmlu_philosophy",
      "task_alias": "philosophy",
      "tag": "mmlu_humanities_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "philosophy",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about philosophy.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_prehistory": {
      "task": "mmlu_prehistory",
      "task_alias": "prehistory",
      "tag": "mmlu_humanities_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "prehistory",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about prehistory.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_professional_accounting": {
      "task": "mmlu_professional_accounting",
      "task_alias": "professional_accounting",
      "tag": "mmlu_other_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "professional_accounting",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about professional accounting.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_professional_law": {
      "task": "mmlu_professional_law",
      "task_alias": "professional_law",
      "tag": "mmlu_humanities_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "professional_law",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about professional law.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_professional_medicine": {
      "task": "mmlu_professional_medicine",
      "task_alias": "professional_medicine",
      "tag": "mmlu_other_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "professional_medicine",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about professional medicine.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_professional_psychology": {
      "task": "mmlu_professional_psychology",
      "task_alias": "professional_psychology",
      "tag": "mmlu_social_sciences_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "professional_psychology",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about professional psychology.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_public_relations": {
      "task": "mmlu_public_relations",
      "task_alias": "public_relations",
      "tag": "mmlu_social_sciences_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "public_relations",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about public relations.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_security_studies": {
      "task": "mmlu_security_studies",
      "task_alias": "security_studies",
      "tag": "mmlu_social_sciences_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "security_studies",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about security studies.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_sociology": {
      "task": "mmlu_sociology",
      "task_alias": "sociology",
      "tag": "mmlu_social_sciences_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "sociology",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about sociology.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_us_foreign_policy": {
      "task": "mmlu_us_foreign_policy",
      "task_alias": "us_foreign_policy",
      "tag": "mmlu_social_sciences_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "us_foreign_policy",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about us foreign policy.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_virology": {
      "task": "mmlu_virology",
      "task_alias": "virology",
      "tag": "mmlu_other_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "virology",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about virology.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "mmlu_world_religions": {
      "task": "mmlu_world_religions",
      "task_alias": "world_religions",
      "tag": "mmlu_humanities_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "world_religions",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about world religions.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "multirc": {
      "task": "multirc",
      "tag": [
        "super-glue-lm-eval-v1"
      ],
      "dataset_path": "super_glue",
      "dataset_name": "multirc",
      "training_split": "train",
      "validation_split": "validation",
      "fewshot_split": "train",
      "doc_to_text": "{{paragraph}}\nQuestion: {{question}}\nAnswer:",
      "doc_to_target": "label",
      "unsafe_code": false,
      "doc_to_choice": "['''{{answer}}\\nIs the answer correct? yes''', '''{{answer}}\\nIs the answer correct? no''']",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc"
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "openbookqa": {
      "task": "openbookqa",
      "dataset_path": "openbookqa",
      "dataset_name": "main",
      "training_split": "train",
      "validation_split": "validation",
      "test_split": "test",
      "fewshot_split": "validation",
      "doc_to_text": "question_stem",
      "doc_to_target": "{{choices.label.index(answerKey.lstrip())}}",
      "unsafe_code": false,
      "doc_to_choice": "{{choices.text}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": true,
      "doc_to_decontamination_query": "question_stem",
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "piqa": {
      "task": "piqa",
      "dataset_path": "baber/piqa",
      "training_split": "train",
      "validation_split": "validation",
      "fewshot_split": "train",
      "doc_to_text": "Question: {{goal}}\nAnswer:",
      "doc_to_target": "label",
      "unsafe_code": false,
      "doc_to_choice": "{{[sol1, sol2]}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": true,
      "doc_to_decontamination_query": "goal",
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "qqp": {
      "task": "qqp",
      "tag": "glue",
      "dataset_path": "nyu-mll/glue",
      "dataset_name": "qqp",
      "training_split": "train",
      "validation_split": "validation",
      "fewshot_split": "train",
      "doc_to_text": "Question 1: {{question1}}\nQuestion 2: {{question2}}\nQuestion: Do both questions ask the same thing?\nAnswer:",
      "doc_to_target": "label",
      "unsafe_code": false,
      "doc_to_choice": [
        "no",
        "yes"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc"
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "race": {
      "task": "race",
      "dataset_path": "EleutherAI/race",
      "dataset_name": "high",
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "def doc_to_text(doc):\n    text = \"Article: \" + doc[\"article\"] + \"\\n\\n\"\n    for problem in process_ast(doc[\"problems\"])[:-1]:\n        if problem[\"question\"][-6:] == \"  _  .\":\n            text += problem[\"question\"][-5:] + get_answer_option(problem) + \"\\n\"\n        else:\n            question = \"Question: \" + problem[\"question\"] + \"\\n\"\n            answer = \"Answer: \" + get_answer_option(problem) + \"\\n\"\n            text += question + answer\n    text += last_problem(doc)[\"question\"]\n    return text\n",
      "doc_to_target": "def doc_to_target(doc):\n    letter_to_num = {\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3}\n    answer = letter_to_num[last_problem(doc)[\"answer\"]]\n    return answer\n",
      "unsafe_code": false,
      "doc_to_choice": "def doc_to_choice(doc):\n    problem = last_problem(doc)\n    choices = [problem[\"options\"][i] for i in range(4)]\n    return choices\n",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "sciq": {
      "task": "sciq",
      "dataset_path": "sciq",
      "training_split": "train",
      "validation_split": "validation",
      "test_split": "test",
      "fewshot_split": "validation",
      "doc_to_text": "{{support.lstrip()}}\nQuestion: {{question}}\nAnswer:",
      "doc_to_target": 3,
      "unsafe_code": false,
      "doc_to_choice": "{{[distractor1, distractor2, distractor3, correct_answer]}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": true,
      "doc_to_decontamination_query": "{{support}} {{question}}",
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "social_iqa": {
      "task": "social_iqa",
      "dataset_path": "jet-ai/social_i_qa",
      "training_split": "train",
      "validation_split": "validation",
      "fewshot_split": "train",
      "doc_to_text": "Q: {{context}} {{question}}\nA:",
      "doc_to_target": "{{ (label|int) - 1 }}",
      "unsafe_code": false,
      "doc_to_choice": "{{[answerA, answerB, answerC]}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    },
    "winogrande": {
      "task": "winogrande",
      "dataset_path": "winogrande",
      "dataset_name": "winogrande_xl",
      "training_split": "train",
      "validation_split": "validation",
      "fewshot_split": "train",
      "doc_to_text": "def doc_to_text(doc):\n    answer_to_num = {\"1\": 0, \"2\": 1}\n    return answer_to_num[doc[\"answer\"]]\n",
      "doc_to_target": "def doc_to_target(doc):\n    idx = doc[\"sentence\"].index(\"_\") + 1\n    return doc[\"sentence\"][idx:].strip()\n",
      "unsafe_code": false,
      "doc_to_choice": "def doc_to_choice(doc):\n    idx = doc[\"sentence\"].index(\"_\")\n    options = [doc[\"option1\"], doc[\"option2\"]]\n    return [doc[\"sentence\"][:idx] + opt for opt in options]\n",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": true,
      "doc_to_decontamination_query": "sentence",
      "metadata": {
        "version": 1.0,
        "pretrained": "models/qwen3-0.6B",
        "tensor_parallel_size": 1,
        "dtype": "auto",
        "gpu_memory_utilization": 0.8,
        "data_parallel_size": 1,
        "max_model_len": 2048
      }
    }
  },
  "versions": {
    "arc_easy": 1.0,
    "copa": 1.0,
    "hellaswag": 1.0,
    "lambada_standard": 1.0,
    "logiqa": 1.0,
    "mmlu": 2,
    "mmlu_abstract_algebra": 1.0,
    "mmlu_anatomy": 1.0,
    "mmlu_astronomy": 1.0,
    "mmlu_business_ethics": 1.0,
    "mmlu_clinical_knowledge": 1.0,
    "mmlu_college_biology": 1.0,
    "mmlu_college_chemistry": 1.0,
    "mmlu_college_computer_science": 1.0,
    "mmlu_college_mathematics": 1.0,
    "mmlu_college_medicine": 1.0,
    "mmlu_college_physics": 1.0,
    "mmlu_computer_security": 1.0,
    "mmlu_conceptual_physics": 1.0,
    "mmlu_econometrics": 1.0,
    "mmlu_electrical_engineering": 1.0,
    "mmlu_elementary_mathematics": 1.0,
    "mmlu_formal_logic": 1.0,
    "mmlu_global_facts": 1.0,
    "mmlu_high_school_biology": 1.0,
    "mmlu_high_school_chemistry": 1.0,
    "mmlu_high_school_computer_science": 1.0,
    "mmlu_high_school_european_history": 1.0,
    "mmlu_high_school_geography": 1.0,
    "mmlu_high_school_government_and_politics": 1.0,
    "mmlu_high_school_macroeconomics": 1.0,
    "mmlu_high_school_mathematics": 1.0,
    "mmlu_high_school_microeconomics": 1.0,
    "mmlu_high_school_physics": 1.0,
    "mmlu_high_school_psychology": 1.0,
    "mmlu_high_school_statistics": 1.0,
    "mmlu_high_school_us_history": 1.0,
    "mmlu_high_school_world_history": 1.0,
    "mmlu_human_aging": 1.0,
    "mmlu_human_sexuality": 1.0,
    "mmlu_humanities": 2,
    "mmlu_international_law": 1.0,
    "mmlu_jurisprudence": 1.0,
    "mmlu_logical_fallacies": 1.0,
    "mmlu_machine_learning": 1.0,
    "mmlu_management": 1.0,
    "mmlu_marketing": 1.0,
    "mmlu_medical_genetics": 1.0,
    "mmlu_miscellaneous": 1.0,
    "mmlu_moral_disputes": 1.0,
    "mmlu_moral_scenarios": 1.0,
    "mmlu_nutrition": 1.0,
    "mmlu_other": 2,
    "mmlu_philosophy": 1.0,
    "mmlu_prehistory": 1.0,
    "mmlu_professional_accounting": 1.0,
    "mmlu_professional_law": 1.0,
    "mmlu_professional_medicine": 1.0,
    "mmlu_professional_psychology": 1.0,
    "mmlu_public_relations": 1.0,
    "mmlu_security_studies": 1.0,
    "mmlu_social_sciences": 2,
    "mmlu_sociology": 1.0,
    "mmlu_stem": 2,
    "mmlu_us_foreign_policy": 1.0,
    "mmlu_virology": 1.0,
    "mmlu_world_religions": 1.0,
    "multirc": 2.0,
    "openbookqa": 1.0,
    "piqa": 1.0,
    "qqp": 2.0,
    "race": 2.0,
    "sciq": 1.0,
    "social_iqa": 0.0,
    "winogrande": 1.0
  },
  "n-shot": {
    "arc_easy": 0,
    "copa": 0,
    "hellaswag": 0,
    "lambada_standard": 0,
    "logiqa": 0,
    "mmlu_abstract_algebra": 0,
    "mmlu_anatomy": 0,
    "mmlu_astronomy": 0,
    "mmlu_business_ethics": 0,
    "mmlu_clinical_knowledge": 0,
    "mmlu_college_biology": 0,
    "mmlu_college_chemistry": 0,
    "mmlu_college_computer_science": 0,
    "mmlu_college_mathematics": 0,
    "mmlu_college_medicine": 0,
    "mmlu_college_physics": 0,
    "mmlu_computer_security": 0,
    "mmlu_conceptual_physics": 0,
    "mmlu_econometrics": 0,
    "mmlu_electrical_engineering": 0,
    "mmlu_elementary_mathematics": 0,
    "mmlu_formal_logic": 0,
    "mmlu_global_facts": 0,
    "mmlu_high_school_biology": 0,
    "mmlu_high_school_chemistry": 0,
    "mmlu_high_school_computer_science": 0,
    "mmlu_high_school_european_history": 0,
    "mmlu_high_school_geography": 0,
    "mmlu_high_school_government_and_politics": 0,
    "mmlu_high_school_macroeconomics": 0,
    "mmlu_high_school_mathematics": 0,
    "mmlu_high_school_microeconomics": 0,
    "mmlu_high_school_physics": 0,
    "mmlu_high_school_psychology": 0,
    "mmlu_high_school_statistics": 0,
    "mmlu_high_school_us_history": 0,
    "mmlu_high_school_world_history": 0,
    "mmlu_human_aging": 0,
    "mmlu_human_sexuality": 0,
    "mmlu_international_law": 0,
    "mmlu_jurisprudence": 0,
    "mmlu_logical_fallacies": 0,
    "mmlu_machine_learning": 0,
    "mmlu_management": 0,
    "mmlu_marketing": 0,
    "mmlu_medical_genetics": 0,
    "mmlu_miscellaneous": 0,
    "mmlu_moral_disputes": 0,
    "mmlu_moral_scenarios": 0,
    "mmlu_nutrition": 0,
    "mmlu_philosophy": 0,
    "mmlu_prehistory": 0,
    "mmlu_professional_accounting": 0,
    "mmlu_professional_law": 0,
    "mmlu_professional_medicine": 0,
    "mmlu_professional_psychology": 0,
    "mmlu_public_relations": 0,
    "mmlu_security_studies": 0,
    "mmlu_sociology": 0,
    "mmlu_us_foreign_policy": 0,
    "mmlu_virology": 0,
    "mmlu_world_religions": 0,
    "multirc": 0,
    "openbookqa": 0,
    "piqa": 0,
    "qqp": 0,
    "race": 0,
    "sciq": 0,
    "social_iqa": 0,
    "winogrande": 0
  },
  "higher_is_better": {
    "arc_easy": {
      "acc_norm": true
    },
    "copa": {
      "acc": true
    },
    "hellaswag": {
      "acc_norm": true
    },
    "lambada_standard": {
      "acc": true
    },
    "logiqa": {
      "acc_norm": true
    },
    "mmlu": {
      "acc": true
    },
    "mmlu_abstract_algebra": {
      "acc": true
    },
    "mmlu_anatomy": {
      "acc": true
    },
    "mmlu_astronomy": {
      "acc": true
    },
    "mmlu_business_ethics": {
      "acc": true
    },
    "mmlu_clinical_knowledge": {
      "acc": true
    },
    "mmlu_college_biology": {
      "acc": true
    },
    "mmlu_college_chemistry": {
      "acc": true
    },
    "mmlu_college_computer_science": {
      "acc": true
    },
    "mmlu_college_mathematics": {
      "acc": true
    },
    "mmlu_college_medicine": {
      "acc": true
    },
    "mmlu_college_physics": {
      "acc": true
    },
    "mmlu_computer_security": {
      "acc": true
    },
    "mmlu_conceptual_physics": {
      "acc": true
    },
    "mmlu_econometrics": {
      "acc": true
    },
    "mmlu_electrical_engineering": {
      "acc": true
    },
    "mmlu_elementary_mathematics": {
      "acc": true
    },
    "mmlu_formal_logic": {
      "acc": true
    },
    "mmlu_global_facts": {
      "acc": true
    },
    "mmlu_high_school_biology": {
      "acc": true
    },
    "mmlu_high_school_chemistry": {
      "acc": true
    },
    "mmlu_high_school_computer_science": {
      "acc": true
    },
    "mmlu_high_school_european_history": {
      "acc": true
    },
    "mmlu_high_school_geography": {
      "acc": true
    },
    "mmlu_high_school_government_and_politics": {
      "acc": true
    },
    "mmlu_high_school_macroeconomics": {
      "acc": true
    },
    "mmlu_high_school_mathematics": {
      "acc": true
    },
    "mmlu_high_school_microeconomics": {
      "acc": true
    },
    "mmlu_high_school_physics": {
      "acc": true
    },
    "mmlu_high_school_psychology": {
      "acc": true
    },
    "mmlu_high_school_statistics": {
      "acc": true
    },
    "mmlu_high_school_us_history": {
      "acc": true
    },
    "mmlu_high_school_world_history": {
      "acc": true
    },
    "mmlu_human_aging": {
      "acc": true
    },
    "mmlu_human_sexuality": {
      "acc": true
    },
    "mmlu_humanities": {
      "acc": true
    },
    "mmlu_international_law": {
      "acc": true
    },
    "mmlu_jurisprudence": {
      "acc": true
    },
    "mmlu_logical_fallacies": {
      "acc": true
    },
    "mmlu_machine_learning": {
      "acc": true
    },
    "mmlu_management": {
      "acc": true
    },
    "mmlu_marketing": {
      "acc": true
    },
    "mmlu_medical_genetics": {
      "acc": true
    },
    "mmlu_miscellaneous": {
      "acc": true
    },
    "mmlu_moral_disputes": {
      "acc": true
    },
    "mmlu_moral_scenarios": {
      "acc": true
    },
    "mmlu_nutrition": {
      "acc": true
    },
    "mmlu_other": {
      "acc": true
    },
    "mmlu_philosophy": {
      "acc": true
    },
    "mmlu_prehistory": {
      "acc": true
    },
    "mmlu_professional_accounting": {
      "acc": true
    },
    "mmlu_professional_law": {
      "acc": true
    },
    "mmlu_professional_medicine": {
      "acc": true
    },
    "mmlu_professional_psychology": {
      "acc": true
    },
    "mmlu_public_relations": {
      "acc": true
    },
    "mmlu_security_studies": {
      "acc": true
    },
    "mmlu_social_sciences": {
      "acc": true
    },
    "mmlu_sociology": {
      "acc": true
    },
    "mmlu_stem": {
      "acc": true
    },
    "mmlu_us_foreign_policy": {
      "acc": true
    },
    "mmlu_virology": {
      "acc": true
    },
    "mmlu_world_religions": {
      "acc": true
    },
    "multirc": {
      "acc": true
    },
    "openbookqa": {
      "acc_norm": true
    },
    "piqa": {
      "acc_norm": true
    },
    "qqp": {
      "acc": true
    },
    "race": {
      "acc": true
    },
    "sciq": {
      "acc_norm": true
    },
    "social_iqa": {
      "acc": true
    },
    "winogrande": {
      "acc": true
    }
  },
  "n-samples": {
    "mmlu_abstract_algebra": {
      "original": 100,
      "effective": 10
    },
    "mmlu_anatomy": {
      "original": 135,
      "effective": 10
    },
    "mmlu_astronomy": {
      "original": 152,
      "effective": 10
    },
    "mmlu_college_biology": {
      "original": 144,
      "effective": 10
    },
    "mmlu_college_chemistry": {
      "original": 100,
      "effective": 10
    },
    "mmlu_college_computer_science": {
      "original": 100,
      "effective": 10
    },
    "mmlu_college_mathematics": {
      "original": 100,
      "effective": 10
    },
    "mmlu_college_physics": {
      "original": 102,
      "effective": 10
    },
    "mmlu_computer_security": {
      "original": 100,
      "effective": 10
    },
    "mmlu_conceptual_physics": {
      "original": 235,
      "effective": 10
    },
    "mmlu_electrical_engineering": {
      "original": 145,
      "effective": 10
    },
    "mmlu_elementary_mathematics": {
      "original": 378,
      "effective": 10
    },
    "mmlu_high_school_biology": {
      "original": 310,
      "effective": 10
    },
    "mmlu_high_school_chemistry": {
      "original": 203,
      "effective": 10
    },
    "mmlu_high_school_computer_science": {
      "original": 100,
      "effective": 10
    },
    "mmlu_high_school_mathematics": {
      "original": 270,
      "effective": 10
    },
    "mmlu_high_school_physics": {
      "original": 151,
      "effective": 10
    },
    "mmlu_high_school_statistics": {
      "original": 216,
      "effective": 10
    },
    "mmlu_machine_learning": {
      "original": 112,
      "effective": 10
    },
    "mmlu_business_ethics": {
      "original": 100,
      "effective": 10
    },
    "mmlu_clinical_knowledge": {
      "original": 265,
      "effective": 10
    },
    "mmlu_college_medicine": {
      "original": 173,
      "effective": 10
    },
    "mmlu_global_facts": {
      "original": 100,
      "effective": 10
    },
    "mmlu_human_aging": {
      "original": 223,
      "effective": 10
    },
    "mmlu_management": {
      "original": 103,
      "effective": 10
    },
    "mmlu_marketing": {
      "original": 234,
      "effective": 10
    },
    "mmlu_medical_genetics": {
      "original": 100,
      "effective": 10
    },
    "mmlu_miscellaneous": {
      "original": 783,
      "effective": 10
    },
    "mmlu_nutrition": {
      "original": 306,
      "effective": 10
    },
    "mmlu_professional_accounting": {
      "original": 282,
      "effective": 10
    },
    "mmlu_professional_medicine": {
      "original": 272,
      "effective": 10
    },
    "mmlu_virology": {
      "original": 166,
      "effective": 10
    },
    "mmlu_econometrics": {
      "original": 114,
      "effective": 10
    },
    "mmlu_high_school_geography": {
      "original": 198,
      "effective": 10
    },
    "mmlu_high_school_government_and_politics": {
      "original": 193,
      "effective": 10
    },
    "mmlu_high_school_macroeconomics": {
      "original": 390,
      "effective": 10
    },
    "mmlu_high_school_microeconomics": {
      "original": 238,
      "effective": 10
    },
    "mmlu_high_school_psychology": {
      "original": 545,
      "effective": 10
    },
    "mmlu_human_sexuality": {
      "original": 131,
      "effective": 10
    },
    "mmlu_professional_psychology": {
      "original": 612,
      "effective": 10
    },
    "mmlu_public_relations": {
      "original": 110,
      "effective": 10
    },
    "mmlu_security_studies": {
      "original": 245,
      "effective": 10
    },
    "mmlu_sociology": {
      "original": 201,
      "effective": 10
    },
    "mmlu_us_foreign_policy": {
      "original": 100,
      "effective": 10
    },
    "mmlu_formal_logic": {
      "original": 126,
      "effective": 10
    },
    "mmlu_high_school_european_history": {
      "original": 165,
      "effective": 10
    },
    "mmlu_high_school_us_history": {
      "original": 204,
      "effective": 10
    },
    "mmlu_high_school_world_history": {
      "original": 237,
      "effective": 10
    },
    "mmlu_international_law": {
      "original": 121,
      "effective": 10
    },
    "mmlu_jurisprudence": {
      "original": 108,
      "effective": 10
    },
    "mmlu_logical_fallacies": {
      "original": 163,
      "effective": 10
    },
    "mmlu_moral_disputes": {
      "original": 346,
      "effective": 10
    },
    "mmlu_moral_scenarios": {
      "original": 895,
      "effective": 10
    },
    "mmlu_philosophy": {
      "original": 311,
      "effective": 10
    },
    "mmlu_prehistory": {
      "original": 324,
      "effective": 10
    },
    "mmlu_professional_law": {
      "original": 1534,
      "effective": 10
    },
    "mmlu_world_religions": {
      "original": 171,
      "effective": 10
    },
    "qqp": {
      "original": 40430,
      "effective": 10
    },
    "copa": {
      "original": 100,
      "effective": 10
    },
    "multirc": {
      "original": 4848,
      "effective": 10
    },
    "logiqa": {
      "original": 651,
      "effective": 10
    },
    "arc_easy": {
      "original": 2376,
      "effective": 10
    },
    "sciq": {
      "original": 1000,
      "effective": 10
    },
    "piqa": {
      "original": 1838,
      "effective": 10
    },
    "lambada_standard": {
      "original": 5153,
      "effective": 10
    },
    "hellaswag": {
      "original": 10042,
      "effective": 10
    },
    "social_iqa": {
      "original": 1954,
      "effective": 10
    },
    "openbookqa": {
      "original": 500,
      "effective": 10
    },
    "race": {
      "original": 1045,
      "effective": 10
    },
    "winogrande": {
      "original": 1267,
      "effective": 10
    }
  },
  "config": {
    "model": "vllm",
    "model_args": {
      "pretrained": "models/qwen3-0.6B",
      "tensor_parallel_size": 1,
      "dtype": "auto",
      "gpu_memory_utilization": 0.8,
      "data_parallel_size": 1,
      "max_model_len": 2048
    },
    "batch_size": "auto",
    "batch_sizes": [],
    "device": "cuda:0",
    "use_cache": null,
    "limit": 10.0,
    "bootstrap_iters": 100000,
    "gen_kwargs": null,
    "random_seed": 0,
    "numpy_seed": 1234,
    "torch_seed": 1234,
    "fewshot_seed": 1234
  },
  "git_hash": "b315ef3b",
  "date": 1764751256.6839347,
  "pretty_env_info": "PyTorch version: 2.9.0+cu126\nIs debug build: False\nCUDA used to build PyTorch: 12.6\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.4 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.35\n\nPython version: 3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0] (64-bit runtime)\nPython platform: Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.6.20\nCUDA_MODULE_LOADING set to: \nGPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090\nNvidia driver version: 581.04\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.3.0\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.3.0\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.3.0\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.3.0\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.3.0\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.3.0\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.3.0\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.3.0\nIs XPU available: False\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        46 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               32\nOn-line CPU(s) list:                  0-31\nVendor ID:                            GenuineIntel\nModel name:                           Intel(R) Xeon(R) Gold 6246R CPU @ 3.40GHz\nCPU family:                           6\nModel:                                85\nThread(s) per core:                   2\nCore(s) per socket:                   16\nSocket(s):                            1\nStepping:                             7\nBogoMIPS:                             6784.05\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology tsc_reliable nonstop_tsc cpuid pni pclmulqdq vmx ssse3 fma cx16 pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx512_vnni md_clear flush_l1d arch_capabilities\nVirtualization:                       VT-x\nHypervisor vendor:                    Microsoft\nVirtualization type:                  full\nL1d cache:                            512 KiB (16 instances)\nL1i cache:                            512 KiB (16 instances)\nL2 cache:                             16 MiB (16 instances)\nL3 cache:                             35.8 MiB (1 instance)\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          KVM: Mitigation: VMX disabled\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Mitigation; Clear CPU buffers; SMT Host state unknown\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Mitigation; Enhanced IBRS\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI SW loop, KVM SW loop\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Mitigation; TSX disabled\n\nVersions of relevant libraries:\n[pip3] mypy_extensions==1.1.0\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.6.4.1\n[pip3] nvidia-cuda-cupti-cu12==12.6.80\n[pip3] nvidia-cuda-nvrtc-cu12==12.6.77\n[pip3] nvidia-cuda-runtime-cu12==12.6.77\n[pip3] nvidia-cudnn-cu12==9.10.2.21\n[pip3] nvidia-cudnn-frontend==1.16.0\n[pip3] nvidia-cufft-cu12==11.3.0.4\n[pip3] nvidia-curand-cu12==10.3.7.77\n[pip3] nvidia-cusolver-cu12==11.7.1.2\n[pip3] nvidia-cusparse-cu12==12.5.4.2\n[pip3] nvidia-cusparselt-cu12==0.7.1\n[pip3] nvidia-nccl-cu12==2.27.5\n[pip3] nvidia-nvjitlink-cu12==12.6.85\n[pip3] nvidia-nvtx-cu12==12.6.77\n[pip3] torch==2.9.0+cu126\n[pip3] torchaudio==2.9.0+cu126\n[pip3] torchvision==0.24.0+cu126\n[pip3] triton==3.5.0\n[conda] Could not collect",
  "transformers_version": "4.57.3",
  "lm_eval_version": "0.4.9.2",
  "upper_git_hash": null,
  "tokenizer_pad_token": [
    "<|endoftext|>",
    "151643"
  ],
  "tokenizer_eos_token": [
    "<|im_end|>",
    "151645"
  ],
  "tokenizer_bos_token": [
    null,
    "None"
  ],
  "eot_token_id": 151645,
  "max_length": 2048,
  "task_hashes": {
    "mmlu_abstract_algebra": "fba6e2c6babf94dfbe2b8f9d4ad64f435673c18ee279c770a93b40da0d47d88e",
    "mmlu_anatomy": "00f8499baecbd91c93fa31c6de5be4bcabc99d97ad9a6aa3cf1f275c381b14dd",
    "mmlu_astronomy": "c3484982388d1eafdda80811edaae23f128a863a3875a48c931b0f925265e656",
    "mmlu_college_biology": "c8bdef4ad9245700004a7b197ac76fa2287097c92f938b9bb60450f762e70276",
    "mmlu_college_chemistry": "a0c3479ec6619d86a2b19539a56e250c65ffcb7e4896fdba76e14a226d8f4eb5",
    "mmlu_college_computer_science": "c57f6500395c8d958a1d9ba4c751c09f930f8976d587d57e5621a103a6ce378a",
    "mmlu_college_mathematics": "9ccc2e6f45b2a55ab1bd1655083f000c9c2eb79fe2046aa3a5791f176edd61ae",
    "mmlu_college_physics": "60d65b38c64d48a152950670131a1e2d1361419eed654ca85b668ce58a0f85eb",
    "mmlu_computer_security": "e8fc7b7d14ab99993f19d5713aed0a23a0abef8f42e146093438a64f423332dd",
    "mmlu_conceptual_physics": "810c7c3ddae91df516e99f124b49585b2b7a4a8702b05066542f6db279bcf924",
    "mmlu_electrical_engineering": "4d756117a5dd8e21c310d041e61157fc31b18ae623fe3b68e877425eccce19eb",
    "mmlu_elementary_mathematics": "d0648231fd5cfb33cd93836efcdf4d8960379e5d5550bcfca351604ee01da275",
    "mmlu_high_school_biology": "31d7f1c2ca9cf9e27b1a3397bc0e01845e1bce8e11e9987851c2bfd3557d4739",
    "mmlu_high_school_chemistry": "3a3af0e1adb9df50dc7c39a2686430087b2d9d851e8bf2097defff80f418f5a7",
    "mmlu_high_school_computer_science": "fec4561e3b8fb15fb936d2a697b020996ddacd60fe3edba7e4b6a450bb9f4e42",
    "mmlu_high_school_mathematics": "08f3e5ff346c980da07b3541a18ec4a3d80b68548327edb7ffb76abb2c478061",
    "mmlu_high_school_physics": "865f62db616744a27f75a098b6a50b381158703dc66952a79ae2ea58b722b122",
    "mmlu_high_school_statistics": "c8ab5ac293fce51457ec42641bb895e1f3028c0a89d45070ed186eb9ba2e5b41",
    "mmlu_machine_learning": "9b40f4c2ec4be407a2d1bdeafb481939222d27e841ca92b76078ef64841ef5a9",
    "mmlu_business_ethics": "8d8d51e3fffe3d7418d874b58fdebc6d3b5dc6c79d1cbc0b052404f69e4716f3",
    "mmlu_clinical_knowledge": "8fa36b9ad204b7f1f058d8bdaada6fa5c70729fb2786f39f84d1e7ee5c08bf92",
    "mmlu_college_medicine": "58a6414790168f709058c4cadaf00053eaa18206b8bc0e4bfd20b524687d5543",
    "mmlu_global_facts": "978095bae5c19b867fe5afc3151c4bdc7c91fdce7371a03f0cb66c03303f1806",
    "mmlu_human_aging": "3f7cb6b0e3bf7887a18f481910710d5569ae8ee98b2acce7fcf6888d8ef609db",
    "mmlu_management": "e8e0a549e293fb0417e3ee9dbcb511f72c5e44378ad5c8f03121a304a5dc0e60",
    "mmlu_marketing": "2b13a2341f22208b5e5958987ef91a9f53345769917e7dcc19b77ecc9185ec4e",
    "mmlu_medical_genetics": "cb452178b1ab63d7d2ec72923aface1ee51c8f8145c77f0a33ac97447caa2061",
    "mmlu_miscellaneous": "c5d081a591d7a43b3e0168982337a0f25bd377e6db7b7f9d2fd0dbda0742223c",
    "mmlu_nutrition": "217d0d7927ad43cbc5d1738a790e871f101c109287af78198b714980a76ce320",
    "mmlu_professional_accounting": "52c6a178788dbbb25682687eb768d955ce942b9dfe587b10a2d40f10ee89f8a2",
    "mmlu_professional_medicine": "7e3468e121fa3dd9a9aad42165707c982a07ef8c87ba7e336906099d057cb2f7",
    "mmlu_virology": "539bd7e2b3f03495106dd4251ef5b90d77505d9a9ee18a00d46312d7f56927c2",
    "mmlu_econometrics": "03ee3d624bfa2483a3365f735e22e1f7aa44ac0e8b53295e74af95252977049f",
    "mmlu_high_school_geography": "1b1ca135e703b7e633b395cf4428006e2a58d8096d301e784d0010853aff8f1c",
    "mmlu_high_school_government_and_politics": "b98d42bb6f65b18286654952ce478c3b7404bf999530456cc420bb7d173a3127",
    "mmlu_high_school_macroeconomics": "7d77764130de41e1ec3eb2dec4ac9b30438d1971445cb2ad0c73a4164ae850e6",
    "mmlu_high_school_microeconomics": "b8a68c06c004f5bdcbb750219e1205ec381600db4626a01a685c741e9b64a6d3",
    "mmlu_high_school_psychology": "87ba7f11082c553257c71e40b26cfe8b8e81443c0f7604fc069aef3aa679ebaa",
    "mmlu_human_sexuality": "b9881227b48c0f576faba71a53a012aae672095a79e4dd729ffe9933db850ca6",
    "mmlu_professional_psychology": "e362546df025e7bb0dc0685b48f6ec717d69cb493c62b4f60b54a64d18062ea7",
    "mmlu_public_relations": "8be11b71ad34e4e05fea8179af86459ce11c16e9fb1ad89656e8ddaecdcc7ba1",
    "mmlu_security_studies": "bf1562a11a5452dd4040e9535d7be611e50f1118148eeb5782b0816288fd7f8d",
    "mmlu_sociology": "7f543d1c870eb8d14f7c02a6750d53dfe6ffa117bf2803e9a9d2364ebcf7b781",
    "mmlu_us_foreign_policy": "51fda86503b5c35e2e5c273271b91be9ed25fdf6e5888266febd6e169ce037a0",
    "mmlu_formal_logic": "0b55222ef682d9cccda307fea075c748226c30a2f0b3161ec039fb794d650223",
    "mmlu_high_school_european_history": "e51a03c09b0f6a1b76e14cd6da7c66c2708e0781cc4bdfeb40e55c4c84b1c5ea",
    "mmlu_high_school_us_history": "0a00b038e65ac2383e0310a7974016e3c80f56714a095dc28ad6a8b7349ed21a",
    "mmlu_high_school_world_history": "348f82d22778d06bea63a38a1bf71b9967aaf932407572e8f262438cabe93435",
    "mmlu_international_law": "599b7a185053f3093bec7d4f582323b7f8600a0ffbd5398be3fefb00b9184b5d",
    "mmlu_jurisprudence": "da8ae9eb71535daebd0a4a82f223812e626d3843abfaf7bc419d0e36e3bd7010",
    "mmlu_logical_fallacies": "5aeff3303b29053dd12b8e507b04c8a7de5afc5dc685b32e6d6159ddceb89391",
    "mmlu_moral_disputes": "ae04633eeb730f4be3ad4fc1722f736b7a968c6eae9c6876557517a73d92dc2e",
    "mmlu_moral_scenarios": "a7612f7ba67605a800cea639480201f9f9d038561ad6b75f2b0215f429d36129",
    "mmlu_philosophy": "bd44f1a050998ac4df8f2ed97a402830da14d176c3e8ccc9275497956590a0be",
    "mmlu_prehistory": "3b15ae6b4529d1d1ccb8da338f1971cf60737ad16d54a21c21610ec88e5065f1",
    "mmlu_professional_law": "aa085739e5f77870db3d474ad86657fe92ce5817797fa6973bf6c027ba35b57c",
    "mmlu_world_religions": "6bf65060a364ab5662b846023766f3b418b358f504752b74afb3b4041bf9d77e",
    "qqp": "1cdaac18adbf2f22e3f0cb2a8f774b3f8e7d6e33b78961e7d1760c48e8497c9c",
    "copa": "191ce6d24a55f1efed63d5972e01ac439767476c3d424b26071055082d528deb",
    "multirc": "014b9dc895525ddd8644744938a07206d68714b3d0bc4127b1b031f7be4cc8b0",
    "logiqa": "40fc8d4539316bdffaa55cd59803563d1e4ae7d0a58357676672c25ebf40538d",
    "arc_easy": "e742a2ea5a5c639814f16ab804f2a74e6bca906472b004eefdccb095b28a5825",
    "sciq": "d9e7bd369d86a863d42ac589ec5e9b895fc058fab13d79465ce5758081a998cc",
    "piqa": "608460916b58fe899a23c5a1d751d2bd0778a0835770e60279267206fc60b40a",
    "lambada_standard": "67cc3371ea8b11ad79cf401294e9e86deb78afe2ce1ff6184b9a1271900ee861",
    "hellaswag": "75fdb4abedb3fa5c861e4905ed49701c1c0798886e6ce3f135a945eaa6115547",
    "social_iqa": "3228d8c1129bde21e7da3b72ade6b0d0d937d8b60238fd0ada67c73fa5fedf67",
    "openbookqa": "12a8e6a09e658d9e7d0d299eee655d16369df9596fa2157a777243c70becc7ba",
    "race": "731ada289dec3e1f02e565aa49fab24a336ce7f80e476a8970b1e3039c1445c1",
    "winogrande": "0f127354d3d3034dfa80028a5d60303c31cb4021a0cc3f47a2b6093d74b487b6"
  },
  "model_source": "vllm",
  "model_name": "",
  "model_name_sanitized": "",
  "system_instruction": null,
  "system_instruction_sha": null,
  "fewshot_as_multiturn": null,
  "chat_template": null,
  "chat_template_sha": null,
  "start_time": 14661.165317446,
  "end_time": 15194.118747162,
  "total_evaluation_time_seconds": "532.9534297159989"
}